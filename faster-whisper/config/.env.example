# Faster-Whisper Streaming ASR Configuration
# License: MIT (commercial use allowed)

# =============================================================================
# Model Configuration
# =============================================================================
# Model size: tiny, base, small, medium, large-v2, large-v3
WHISPER_MODEL=large-v3

# Device: cuda, cpu
WHISPER_DEVICE=cuda

# Compute type: float16, int8, int8_float16
# int8 = ~3GB VRAM (recommended)
# float16 = ~6GB VRAM
WHISPER_COMPUTE_TYPE=int8

# =============================================================================
# Server Configuration
# =============================================================================
HOST=0.0.0.0
PORT=4400

# =============================================================================
# Feature Toggles
# =============================================================================
# Speaker diarization (requires HF_TOKEN)
ENABLE_DIARIZATION=true

# Hugging Face token for pyannote
# Get token at: https://huggingface.co/settings/tokens
# Accept license at: https://huggingface.co/pyannote/speaker-diarization-3.1
HF_TOKEN=your_huggingface_token_here

# =============================================================================
# GPU Memory Estimates
# =============================================================================
# faster-whisper large-v3 (INT8):  ~3.0GB
# Silero VAD (ONNX):               ~0.1GB
# Pyannote diarization:            ~1.2GB
# ─────────────────────────────────────────
# Total:                           ~4.3GB
